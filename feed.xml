<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://hansungy.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hansungy.github.io/" rel="alternate" type="text/html" /><updated>2025-06-17T17:04:31+00:00</updated><id>https://hansungy.github.io/feed.xml</id><title type="html">Seungyub Han</title><entry><title type="html">Policy-labeled Preference Learning: Is Preference Enough for RLHF?</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Policy-labeled Preference Learning: Is Preference Enough for RLHF?" /><published>2025-06-15T21:21:59+00:00</published><updated>2025-06-15T21:21:59+00:00</updated><id>https://hansungy.github.io/ppl</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Experiments in high-dimensional continuous control environments demonstrate PPL’s significant improvements in offline RLHF performance and its effectiveness in online settings.</p>]]></content><author><name>Taehyun Cho</name></author><category term="research" /><summary type="html"><![CDATA[We propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Experiments in high-dimensional continuous control environments demonstrate PPL’s significant improvements in offline RLHF performance and its effectiveness in online settings.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/ppl.png" /><media:content medium="image" url="https://hansungy.github.io/images/ppl.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation" /><published>2025-06-10T20:21:59+00:00</published><updated>2025-06-10T20:21:59+00:00</updated><id>https://hansungy.github.io/bellman_unbiasedness</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.</p>]]></content><author><name>Taehyun Cho</name></author><category term="research" /><summary type="html"><![CDATA[Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/statistical_functional.png" /><media:content medium="image" url="https://hansungy.github.io/images/statistical_functional.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees" /><published>2024-10-10T22:21:59+00:00</published><updated>2024-10-10T22:21:59+00:00</updated><id>https://hansungy.github.io/spectral</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.</p>]]></content><author><name>Dohyeong Kim</name></author><category term="research" /><summary type="html"><![CDATA[We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/kim_spectral.png" /><media:content medium="image" url="https://hansungy.github.io/images/kim_spectral.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">D2NAS: Efficient Neural Architecture Search with Performance Improvement and Model Size Reduction for Diverse Tasks</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="D2NAS: Efficient Neural Architecture Search with Performance Improvement and Model Size Reduction for Diverse Tasks" /><published>2024-07-29T22:21:59+00:00</published><updated>2024-07-29T22:21:59+00:00</updated><id>https://hansungy.github.io/d2nas_ieee_access</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We introduce D2NAS, Differential and Diverse NAS, leveraging techniques such as Differentiable ARchiTecture Search (DARTS) and Diverse-task Architecture SearcH (DASH) for architecture discovery.</p>]]></content><author><name>Suengyub Han</name></author><category term="research" /><summary type="html"><![CDATA[We introduce D2NAS, Differential and Diverse NAS, leveraging techniques such as Differentiable ARchiTecture Search (DARTS) and Diverse-task Architecture SearcH (DASH) for architecture discovery.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/d2nas.png" /><media:content medium="image" url="https://hansungy.github.io/images/d2nas.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion" /><published>2023-12-07T22:21:59+00:00</published><updated>2023-12-07T22:21:59+00:00</updated><id>https://hansungy.github.io/pqr</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.</p>]]></content><author><name>Taehyun Cho</name></author><category term="research" /><summary type="html"><![CDATA[We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/pqr.png" /><media:content medium="image" url="https://hansungy.github.io/images/pqr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning" /><published>2023-12-07T20:21:59+00:00</published><updated>2023-12-07T20:21:59+00:00</updated><id>https://hansungy.github.io/spqr</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.</p>]]></content><author><name>Doheok Lee</name></author><category term="research" /><summary type="html"><![CDATA[By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/spqr.png" /><media:content medium="image" url="https://hansungy.github.io/images/spqr.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On the Convergence of Continual Learning with Adaptive Methods</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="On the Convergence of Continual Learning with Adaptive Methods" /><published>2023-08-12T22:21:59+00:00</published><updated>2023-08-12T22:21:59+00:00</updated><id>https://hansungy.github.io/nccl_uai</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.</p>]]></content><author><name>Suengyub Han</name></author><category term="research" /><summary type="html"><![CDATA[In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/nccl_uai.png" /><media:content medium="image" url="https://hansungy.github.io/images/nccl_uai.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Perturbed Quantile Regression for Distributional Reinforcement Learning</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Perturbed Quantile Regression for Distributional Reinforcement Learning" /><published>2022-12-03T22:21:59+00:00</published><updated>2022-12-03T22:21:59+00:00</updated><id>https://hansungy.github.io/pqrl_neurips_workshop</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.</p>]]></content><author><name>Taehyun Cho</name></author><category term="research" /><summary type="html"><![CDATA[We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/pqr_neurips2022_workshop.png" /><media:content medium="image" url="https://hansungy.github.io/images/pqr_neurips2022_workshop.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Adaptive Methods for Nonconvex Continual Learning</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Adaptive Methods for Nonconvex Continual Learning" /><published>2022-12-03T22:21:59+00:00</published><updated>2022-12-03T22:21:59+00:00</updated><id>https://hansungy.github.io/nccl_opt</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.</p>]]></content><author><name>Suengyub Han</name></author><category term="research" /><summary type="html"><![CDATA[We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/nccl_opt_ml.png" /><media:content medium="image" url="https://hansungy.github.io/images/nccl_opt_ml.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Learning to Learn Unlearned Feature for Brain Tumor Segmentation</title><link href="https://hansungy.github.io/" rel="alternate" type="text/html" title="Learning to Learn Unlearned Feature for Brain Tumor Segmentation" /><published>2018-12-08T20:21:59+00:00</published><updated>2018-12-08T20:21:59+00:00</updated><id>https://hansungy.github.io/amt</id><content type="html" xml:base="https://hansungy.github.io/"><![CDATA[<p>One of the difficulties in medical image segmentation is the lack of datasets with proper annotations. To alleviate this problem, we propose active meta-tune to achieve balanced parameters for both glioma and brain metastasis domains within a few steps.</p>]]></content><author><name>Seungyub Han</name></author><category term="research" /><summary type="html"><![CDATA[One of the difficulties in medical image segmentation is the lack of datasets with proper annotations. To alleviate this problem, we propose active meta-tune to achieve balanced parameters for both glioma and brain metastasis domains within a few steps.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://hansungy.github.io/images/amt.png" /><media:content medium="image" url="https://hansungy.github.io/images/amt.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>