<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Seungyub Han</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Seungyub Han">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css">
  <link rel="canonical" href="https://hansungy.github.io/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
 <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr style="padding:0px">
<td style="padding:2.5%;width:60%;vertical-align:middle">
<h1>
Seungyub Han
</h1>
<p>I am a PhD student at the <a href="https://cml.snu.ac.kr//">Communications and Machine Learning Laboratory</a>, part of the <a href="https://ece.snu.ac.kr/">Department of Electrical and Computer Engineering</a> at <a href="https://www.snu.ac.kr/">Seoul National University</a>, where I work on Reinforcement Learning, Robotics, and Deep Learning. My PhD advisor is <a href="https://cml.snu.ac.kr/?page_id=5948">Jungwoo Lee</a>.
</p>
<p>
I have a BS in EE from <a href="https://ece.snu.ac.kr">Seoul National University</a>.
</p>
<p style="text-align:center">
E-mail: <a target="_blank" href="mailto:seungyubhan@snu.ac.kr"> seungyubhan@snu.ac.kr</a>
</p>
<p style="text-align:center">
<a href="https://github.com/hansungy">GitHub</a>  / 
<a href="https://scholar.google.com/citations?user=ot1-XNAAAAAJ&hl=ko&authuser=2">Google Scholar</a>  / 
<a href="https://www.linkedin.com/in/hansungy"> LinkedIn </a>  / 
<a href="/pdfs/cv_hsy.pdf"> CV </a>
</p>
</td>
<td style="padding:2.5%;width:40%;max-width:40%">
<img style="width:100%;max-width:100%" alt="profile photo" src="images/img_crop.jpg">
</td>
</tr>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:100%;vertical-align:middle">
<h2>Publications</h2>
<p>
I'm interested in reinforcment learning, robot learning, optimization, and representation learning.
</p>
</td>
</tr>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/ppl.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Policy-labeled Preference Learning: Is Preference Enough for RLHF?</h3>
<br>
Taehyun Cho, Seokhun Ju, <strong>Seungyub Han</strong>,  Dohyeong Kim, Kyungjae Lee,  Jungwoo Lee<br>
<em>ICML 2025</em>
<span style="font-weight: bold; color: red;"> 	(Spotlight) </span>
<br>
<a href="https://openreview.net/forum?id=qLfo1sef50">paper</a> /
<a href="https://www.arxiv.org/abs/2505.06273">arxiv</a> /
<p></p>
<p>We propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Experiments in high-dimensional continuous control environments demonstrate PPL’s significant improvements in offline RLHF performance and its effectiveness in online settings.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/statistical_functional.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</h3>
<br>
Taehyun Cho, <strong>Seungyub Han</strong>, Seokhun Ju, Dohyeong Kim, Kyungjae Lee,  Jungwoo Lee<br>
<em>ICML 2025</em>
<br>
<a href="https://openreview.net/forum?id=CAvnZQgrLu">paper</a> /
<a href="https://arxiv.org/abs/2407.21260">arxiv</a> /
<p></p>
<p>Our theoretical results show that approximating the infinite-dimensional return distribution with a finite number of moment functionals is the only method to learn the statistical information unbiasedly including nonlinear statistical functional. Second, we propose a provably efficient algorithm, SF-LSVI, achieving a regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/kim_spectral.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees</h3>
<br>
Dohyeong Kim, Taehyun Cho, <strong>Seungyub Han</strong>, Hojun Chung, Kyungjae Lee, Songhwai Oh<br>
<em>NeurIPS 2024</em>
<br>
<a href="https://arxiv.org/abs/2405.18698">arxiv</a> /
<p></p>
<p>We propose a safe RL algorithm with spectral risk constraints, which shows convergence to an optimum in tabular settings.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/d2nas.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>D2NAS: Efficient Neural Architecture Search with Performance Improvement and Model Size Reduction for Diverse Tasks</h3>
<br>
Jungeun Lee, <strong>Seungyub Han</strong>, Jungwoo Lee<br>
<em>IEEE Access</em>
<br>
<a href="https://ieeexplore.ieee.org/abstract/document/10613774">paper</a> /
<p></p>
<p>We introduce D2NAS, Differential and Diverse NAS, leveraging techniques such as Differentiable ARchiTecture Search (DARTS) and Diverse-task Architecture SearcH (DASH) for architecture discovery.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/pqr.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</h3>
<br>
Taehyun Cho, <strong>Seungyub Han</strong>, Heesoo Lee, Kyungjae Lee, Jungwoo Lee<br>
<em>NeurIPS 2023</em>
<br>
<a href="https://openreview.net/forum?id=v8u3EFAyW9">paper</a> /
<a href="https://arxiv.org/abs/2310.16546">arxiv</a> /
<p></p>
<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/spqr.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning</h3>
<br>
Dohyeok Lee, <strong>Seungyub Han</strong>, Taehyun Cho, Jungwoo Lee<br>
<em>NeurIPS 2023</em>
<br>
<a href="https://openreview.net/forum?id=q0sdoFIfNg">paper</a> /
<a href="https://arxiv.org/abs/2401.03137">arxiv</a> /
<a href="https://github.com/dohyeoklee/SPQR">code</a> /
<p></p>
<p>By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/nccl_uai.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>On the Convergence of Continual Learning with Adaptive Methods</h3>
<br>
<strong>Seungyub Han</strong>, Yeongmo Kim, Taehyun Cho, Jungwoo Lee<br>
<em>UAI 2023</em>
<br>
<a href="https://proceedings.mlr.press/v216/han23a.html">paper</a> /
<a href="https://arxiv.org/abs/2404.05555">arxiv</a> /
<p></p>
<p>In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/nccl_opt_ml.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Adaptive Methods for Nonconvex Continual Learning</h3>
<br>
<strong>Seungyub Han</strong>, Yeongmo Kim, Taehyun Cho, Jungwoo Lee<br>
<em>NeurIPS 2022 Optimization for Machine Learning Workshop</em>
<br>
<a href="https://opt-ml.org/oldopt/papers/2022/paper14.pdf">paper</a> /
<p></p>
<p>We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/pqr_neurips2022_workshop.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Perturbed Quantile Regression for Distributional Reinforcement Learning</h3>
<br>
Taehyun Cho, <strong>Seungyub Han</strong>, Heesoo Lee, Kyungjae Lee, Jungwoo Lee<br>
<em>NeurIPS 2022 Deep RL Workshop</em>
<br>
<a href="https://openreview.net/forum?id=-WXCYvc5E-P">paper</a> /
<p></p>
<p>We provide a perturbed distributional Bellman optimality operator by distorting the risk measure in action selection, and prove the convergence and optimality of the proposed method by using the weaker contraction property.</p>
</td>
</tr>
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/amt.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Learning to Learn Unlearned Feature for Brain Tumor Segmentation</h3>
<br>
<strong>Seungyub Han</strong>, Yeongmo Kim, Seokhyeon Ha, Jungwoo Lee, Seunghong Choi<br>
<em>Medical Imaging meets NeurIPS (NeurIPS 2018 Workshop)</em>
<br>
<a href="https://www.doc.ic.ac.uk/~bglocker/public/mednips2018/med-nips_2018_paper_41.pdf">paper</a> /
<a href="https://arxiv.org/abs/2305.08878">arxiv</a> /
<p></p>
<p>One of the difficulties in medical image segmentation is the lack of datasets with proper annotations. To alleviate this problem, we propose active meta-tune to achieve balanced parameters for both glioma and brain metastasis domains within a few steps.</p>
</td>
</tr>
</table>
<p><br>
<br>
<br></p>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:100%;vertical-align:middle">
<h2>Preprints</h2>
</td>
</tr>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
<img src="/images/gat.png" alt="project image" style="width:auto; height:auto; max-width:100%;">
</td>
<td style="padding:2.5%;width:75%;vertical-align:middle">
<h3>Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN</h3>
<br>
Hyeungill Lee, <strong>Seungyub Han</strong>, Jungwoo Lee<br>
<em></em>, 2017<br>
<a href="https://arxiv.org/abs/1705.03387">arxiv</a> /
<p></p>
<p>We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image.</p>
</td>
</tr>
</table>
<p><br>
<br>
<br></p>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:2.5%;width:100%;vertical-align:middle">
<h2>Other Activities</h2>
<p>
</p>
</td>
</tr>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tr>
<td style="padding:0px">
<br>
<p style="text-align:center;font-size:small;">
Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
</p>
</td>
</tr>
</table>
</td>
    </tr>
  </table>
</body>

</html>
